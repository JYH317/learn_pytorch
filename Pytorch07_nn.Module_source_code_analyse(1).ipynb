{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b95be33",
   "metadata": {},
   "source": [
    "# MODULE\n",
    "Base class for all neural network modules.\n",
    "\n",
    "Your models should also subclass this class.\n",
    "\n",
    "Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fb0fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5) #module也可以包含其他module\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return F.relu(self.conv2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b392562b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "Linear(in_features=2, out_features=1, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 1.]], requires_grad=True)\n",
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (1): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply(fn): Applies fn recursively to every submodule (as returned by .children()) as well as self. \n",
    "# Typical use includes initializing the parameters of a model.\n",
    "@torch.no_grad()\n",
    "def init_weights(m): # m为modules\n",
    "    print(m)\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.fill_(1.0)\n",
    "        print(m.weight)\n",
    "net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 1))\n",
    "net.apply(init_weights) # apply最后会作用在自身module上"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee20d26",
   "metadata": {},
   "source": [
    "#### More APIs can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7178ed7d",
   "metadata": {},
   "source": [
    "## Introduce API\n",
    "### Part of the class [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) source code is cited below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4091c4db",
   "metadata": {},
   "source": [
    "#### \\_\\_init__(),   register_buffer(),   register_parameter(),   add_module(),   get_submodule()等函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5782a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ...... # 前面内容省略\n",
    "    \n",
    "    def __init__(self) -> None: # 构造函数\n",
    "        \"\"\"\n",
    "        Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
    "        \"\"\"\n",
    "        torch._C._log_api_usage_once(\"python.nn_module\")\n",
    "\n",
    "        self.training = True  #默认training=True，所以dropout，BN等都默认遵循training=True的情况\n",
    "        self._parameters: Dict[str, Optional[Parameter]] = OrderedDict()  #成员变量储存类型多为字典\n",
    "        self._buffers: Dict[str, Optional[Tensor]] = OrderedDict()\n",
    "        self._non_persistent_buffers_set: Set[str] = set()\n",
    "        self._backward_hooks: Dict[int, Callable] = OrderedDict()\n",
    "        self._is_full_backward_hook = None\n",
    "        self._forward_hooks: Dict[int, Callable] = OrderedDict()\n",
    "        self._forward_pre_hooks: Dict[int, Callable] = OrderedDict()\n",
    "        self._state_dict_hooks: Dict[int, Callable] = OrderedDict()\n",
    "        self._load_state_dict_pre_hooks: Dict[int, Callable] = OrderedDict()\n",
    "        self._modules: Dict[str, Optional['Module']] = OrderedDict()\n",
    "\n",
    "    forward: Callable[..., Any] = _forward_unimplemented\n",
    "\n",
    "        \n",
    "   \n",
    "    def register_buffer(self, name: str, tensor: Optional[Tensor], persistent: bool = True) -> None: # 注册buffer的函数\n",
    "        \n",
    "        # buffer不是模型参数如批归一化的数据的均值方差，不是模型参数，而是数据的一些统计值，但也是模型的一部分（module的一个状态）\n",
    "        # buffer正常是持久的，正常和parameters一起保存在state_dict中\n",
    "        r\"\"\"Adds a buffer to the module.\n",
    "\n",
    "        This is typically used to register a buffer that should not to be\n",
    "        considered a model parameter. For example, BatchNorm's ``running_mean``\n",
    "        is not a parameter, but is part of the module's state. Buffers, by\n",
    "        default, are persistent and will be saved alongside parameters. This\n",
    "        behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
    "        only difference between a persistent buffer and a non-persistent buffer\n",
    "        is that the latter will not be a part of this module's\n",
    "        :attr:`state_dict`.\n",
    "\n",
    "        Buffers can be accessed as attributes using given names.\n",
    "\n",
    "        Args:\n",
    "            name (string): name of the buffer. The buffer can be accessed\n",
    "                from this module using the given name\n",
    "            tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
    "                that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
    "                the buffer is **not** included in the module's :attr:`state_dict`.\n",
    "            persistent (bool): whether the buffer is part of this module's         #是否保存下来\n",
    "                :attr:`state_dict`.\n",
    "\n",
    "        Example::\n",
    "\n",
    "            >>> self.register_buffer('running_mean', torch.zeros(num_features)) # 例子，比如要自己实现batchnorm类等会用到\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "    ...... # 后面内容省略   \n",
    "    \n",
    "    def register_parameter(self, name: str, param: Optional[Parameter]) -> None:   # 比register buffer更常用\n",
    "        r\"\"\"Adds a parameter to the module.\n",
    "\n",
    "        The parameter can be accessed as an attribute using given name.\n",
    "\n",
    "        Args:\n",
    "            name (string): name of the parameter. The parameter can be accessed\n",
    "                from this module using the given name\n",
    "            param (Parameter or None): parameter to be added to the module. If      # parameter类是tensor类的子类\n",
    "                ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
    "                are ignored. If ``None``, the parameter is **not** included in the\n",
    "                module's :attr:`state_dict`.\n",
    "                \n",
    "        Example:: #实现示例\n",
    "        \n",
    "            class GaussianModel(nn.Module):\n",
    "                def __init__(self):\n",
    "                    super(GaussianModel, self).__init__()\n",
    "                    self.register_parameter('mean', nn.Parameter(torch.zeros(1),requires_grad=True)) #输入名称和parameter变量\n",
    "                    ...\n",
    "                def forward(self, x):\n",
    "                    ...               \n",
    "        \"\"\"\n",
    "        # parameter类路径：CLASS torch.nn.parameter.Parameter(data=None, requires_grad=True)\n",
    "        # 模型内部写参数时要用parameter类型，其可以自动加入module的parameters列表中，而不要用tensor类型\n",
    "        \n",
    "        ...... # 后面内容省略 \n",
    "        \n",
    "        \n",
    "    def add_module(self, name: str, module: Optional['Module']) -> None:  \n",
    "        # 向当前module增加子module，最后会储存在self._modules的字典中（ self._modules[name] = module ）\n",
    "        \"\"\" Adds a child module to the current module.\"\"\"       \n",
    "        \n",
    "        ...... # 后面内容省略 \n",
    "        \n",
    "        \n",
    "    def register_module(self, name: str, module: Optional['Module']) -> None:   # 同add_module\n",
    "        r\"\"\"Alias for :func:`add_module`.\"\"\" \n",
    "        self.add_module(name, module)\n",
    "        \n",
    "        ...... # 后面内容省略 \n",
    "        \n",
    "        \n",
    "    def get_submodule(self, target: str) -> \"Module\":   # 找到当前module的子module\n",
    "        \"\"\"\n",
    "        Returns the submodule given by ``target`` if it exists,\n",
    "        otherwise throws an error.\n",
    "\n",
    "        For example, let's say you have an ``nn.Module`` ``A`` that\n",
    "        looks like this:\n",
    "\n",
    "        .. code-block::text\n",
    "\n",
    "            A(\n",
    "                (net_b): Module(\n",
    "                    (net_c): Module(\n",
    "                        (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
    "                    )\n",
    "                    (linear): Linear(in_features=100, out_features=200, bias=True)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
    "        submodule ``net_b``, which itself has two submodules ``net_c``\n",
    "        and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
    "\n",
    "        To check whether or not we have the ``linear`` submodule, we\n",
    "        would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
    "        we have the ``conv`` submodule, we would call\n",
    "        ``get_submodule(\"net_b.net_c.conv\")``.     # 用法示例（用.连接）\n",
    "        \"\"\"\n",
    "        ...... # 后面内容省略 \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c84c954",
   "metadata": {},
   "source": [
    "#### get_parameter()函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a78a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_parameter(self, target: str) -> \"Parameter\":   # 得到parameter，target要将module path写完整\n",
    "        \"\"\"\n",
    "        Returns the parameter given by ``target`` if it exists,\n",
    "        otherwise throws an error.\n",
    "\n",
    "        See the docstring for ``get_submodule`` for a more detailed\n",
    "        explanation of this method's functionality as well as how to\n",
    "        correctly specify ``target``.\n",
    "\n",
    "        Args:\n",
    "            target: The fully-qualified string name of the Parameter\n",
    "                to look for. (See ``get_submodule`` for how to specify a\n",
    "                fully-qualified string.)\n",
    "\n",
    "        Returns:\n",
    "            torch.nn.Parameter: The Parameter referenced by ``target``\n",
    "\n",
    "        Raises:\n",
    "            AttributeError: If the target string references an invalid\n",
    "                path or resolves to something that is not an\n",
    "                ``nn.Parameter``\n",
    "        \"\"\"\n",
    "        module_path, _, param_name = target.rpartition(\".\")  # 语句返回最右边的“.”左侧的字符串，“.”本身，和“.”右侧的字符串\n",
    "        \n",
    "        mod: torch.nn.Module = self.get_submodule(module_path) # 最右边的“.”左侧的字符串即为模块路径，此语句获取模块路径下的子模块赋值给mod\n",
    "\n",
    "        if not hasattr(mod, param_name):  # hasattr判断模块中是否有相应属性，语句判断mod模块中是否有param_name的键名\n",
    "            raise AttributeError(mod._get_name() + \" has no attribute `\"\n",
    "                                 + param_name + \"`\")\n",
    "\n",
    "        param: torch.nn.Parameter = getattr(mod, param_name) #module中有这个参数，将参数赋值给param\n",
    "\n",
    "        if not isinstance(param, torch.nn.Parameter): # 判断param是否是torch.nn.Parameter类的实例，若不是则报错\n",
    "            raise AttributeError(\"`\" + param_name + \"` is not an \"\n",
    "                                 \"nn.Parameter\")\n",
    "\n",
    "        return param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae83d02a",
   "metadata": {},
   "source": [
    "#### get_buffer()函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f949910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_buffer(self, target: str) -> \"Tensor\": # 得到 buffer，target要将module path写完整\n",
    "        \"\"\"\n",
    "        Returns the buffer given by ``target`` if it exists,\n",
    "        otherwise throws an error.\n",
    "\n",
    "        See the docstring for ``get_submodule`` for a more detailed\n",
    "        explanation of this method's functionality as well as how to\n",
    "        correctly specify ``target``.\n",
    "\n",
    "        Args:\n",
    "            target: The fully-qualified string name of the buffer\n",
    "                to look for. (See ``get_submodule`` for how to specify a\n",
    "                fully-qualified string.)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The buffer referenced by ``target``\n",
    "\n",
    "        Raises:\n",
    "            AttributeError: If the target string references an invalid\n",
    "                path or resolves to something that is not a\n",
    "                buffer\n",
    "        \"\"\"\n",
    "        module_path, _, buffer_name = target.rpartition(\".\")\n",
    "\n",
    "        mod: torch.nn.Module = self.get_submodule(module_path)\n",
    "\n",
    "        if not hasattr(mod, buffer_name):\n",
    "            raise AttributeError(mod._get_name() + \" has no attribute `\"\n",
    "                                 + buffer_name + \"`\")\n",
    "\n",
    "        buffer: torch.Tensor = getattr(mod, buffer_name)\n",
    "\n",
    "        if buffer_name not in mod._buffers:  \n",
    "            raise AttributeError(\"`\" + buffer_name + \"` is not a buffer\")\n",
    "        # buffer类型是tensor无法用类型审核，但又要看其是否是一个buffer还是中间计算产生的tensor，就可用mod._buffers的字典去判断\n",
    "            \n",
    "        return buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0372b94e",
   "metadata": {},
   "source": [
    "#### _apply()函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b47769",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _apply(self, fn):  # 操作的对象有三，所有module，parameter和buffer\n",
    "        for module in self.children():\n",
    "            module._apply(fn)    # 对当前模块的所有子模块调用相应fn\n",
    "\n",
    "        def compute_should_use_set_data(tensor, tensor_applied):\n",
    "            if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n",
    "                # If the new tensor has compatible tensor type as the existing tensor,\n",
    "                # the current behavior is to change the tensor in-place using `.data =`,\n",
    "                # and the future behavior is to overwrite the existing tensor. However,\n",
    "                # changing the current behavior is a BC-breaking change, and we want it\n",
    "                # to happen in future releases. So for now we introduce the\n",
    "                # `torch.__future__.get_overwrite_module_params_on_conversion()`\n",
    "                # global flag to let the user control whether they want the future\n",
    "                # behavior of overwriting the existing tensor or not.\n",
    "                return not torch.__future__.get_overwrite_module_params_on_conversion()\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        for key, param in self._parameters.items():\n",
    "            if param is None:\n",
    "                continue\n",
    "            # Tensors stored in modules are graph leaves, and we don't want to\n",
    "            # track autograd history of `param_applied`, so we have to use\n",
    "            # `with torch.no_grad():`\n",
    "            with torch.no_grad():\n",
    "                param_applied = fn(param)    # 对所有参数施加fn\n",
    "            should_use_set_data = compute_should_use_set_data(param, param_applied)\n",
    "            if should_use_set_data:\n",
    "                param.data = param_applied\n",
    "                out_param = param\n",
    "            else:\n",
    "                assert isinstance(param, Parameter)\n",
    "                assert param.is_leaf\n",
    "                out_param = Parameter(param_applied, param.requires_grad)\n",
    "                self._parameters[key] = out_param\n",
    "\n",
    "            if param.grad is not None:\n",
    "                with torch.no_grad():\n",
    "                    grad_applied = fn(param.grad)\n",
    "                should_use_set_data = compute_should_use_set_data(param.grad, grad_applied)\n",
    "                if should_use_set_data:\n",
    "                    out_param.grad.data = grad_applied\n",
    "                else:\n",
    "                    assert param.grad.is_leaf\n",
    "                    out_param.grad = grad_applied.requires_grad_(param.grad.requires_grad)\n",
    "\n",
    "        for key, buf in self._buffers.items():\n",
    "            if buf is not None:\n",
    "                self._buffers[key] = fn(buf)    # 对所有buffer也施加fn\n",
    "   \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7aa16",
   "metadata": {},
   "source": [
    "#### apply()函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb3e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def apply(self: T, fn: Callable[['Module'], None]) -> T:  # 模型初始化参数时有时会用到\n",
    "        r\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
    "        as well as self. Typical use includes initializing the parameters of a model\n",
    "        (see also :ref:`nn-init-doc`).\n",
    "\n",
    "        Args:\n",
    "            fn (:class:`Module` -> None): function to be applied to each submodule\n",
    "\n",
    "        Returns:\n",
    "            Module: self\n",
    "\n",
    "        Example::\n",
    "\n",
    "            >>> @torch.no_grad()\n",
    "            >>> def init_weights(m):\n",
    "            >>>     print(m)\n",
    "            >>>     if type(m) == nn.Linear:\n",
    "            >>>         m.weight.fill_(1.0)\n",
    "            >>>         print(m.weight)\n",
    "            >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
    "            >>> net.apply(init_weights)\n",
    "            Linear(in_features=2, out_features=2, bias=True)\n",
    "            Parameter containing:\n",
    "            tensor([[ 1.,  1.],\n",
    "                    [ 1.,  1.]])\n",
    "            Linear(in_features=2, out_features=2, bias=True)\n",
    "            Parameter containing:\n",
    "            tensor([[ 1.,  1.],\n",
    "                    [ 1.,  1.]])\n",
    "            Sequential(\n",
    "              (0): Linear(in_features=2, out_features=2, bias=True)\n",
    "              (1): Linear(in_features=2, out_features=2, bias=True)\n",
    "            )\n",
    "            Sequential(\n",
    "              (0): Linear(in_features=2, out_features=2, bias=True)\n",
    "              (1): Linear(in_features=2, out_features=2, bias=True)\n",
    "            )\n",
    "        \"\"\"\n",
    "        for module in self.children(): # 遍历子模块并施加相应fn\n",
    "            module.apply(fn)\n",
    "        fn(self)       #注意：对所有子模块作用后再将fn作用在自身模块上\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe92c2db",
   "metadata": {},
   "source": [
    "#### cuda()函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6d6fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def cuda(self: T, device: Optional[Union[int, device]] = None) -> T: # 将所有parameters和buffers移动到gpu上\n",
    "        r\"\"\"Moves all model parameters and buffers to the GPU.\n",
    "\n",
    "        This also makes associated parameters and buffers different objects. So\n",
    "        it should be called before constructing optimizer if the module will\n",
    "        live on GPU while being optimized.\n",
    "\n",
    "        .. note::\n",
    "            This method modifies the module in-place.\n",
    "\n",
    "        Args:\n",
    "            device (int, optional): if specified, all parameters will be\n",
    "                copied to that device\n",
    "\n",
    "        Returns:\n",
    "            Module: self\n",
    "        \"\"\"\n",
    "        return self._apply(lambda t: t.cuda(device)) #_apply作用于所有子模块，parameters和buffers上"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa311b",
   "metadata": {},
   "source": [
    "#### type()函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84339558",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def type(self: T, dst_type: Union[dtype, str]) -> T:\n",
    "        r\"\"\"Casts all parameters and buffers to :attr:`dst_type`.\n",
    "\n",
    "        .. note::\n",
    "            This method modifies the module in-place.\n",
    "\n",
    "        Args:\n",
    "            dst_type (type or string): the desired type\n",
    "\n",
    "        Returns:\n",
    "            Module: self\n",
    "        \"\"\"\n",
    "        return self._apply(lambda t: t.type(dst_type))  #对目标转换成相应datatype，实现过程同cuda(),cpu(),xpu()等函数类似"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6839c6c5",
   "metadata": {},
   "source": [
    "#### to_empty()函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a03a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 几乎不会被用到\n",
    "    def to_empty(self: T, *, device: Union[str, device]) -> T:  # 将模型中所有parameters和buffers移动到同一个device上但不拷贝储存数据\n",
    "        r\"\"\"Moves the parameters and buffers to the specified device without copying storage.\n",
    "\n",
    "        Args:\n",
    "            device (:class:`torch.device`): The desired device of the parameters\n",
    "                and buffers in this module.\n",
    "\n",
    "        Returns:\n",
    "            Module: self\n",
    "        \"\"\"\n",
    "        return self._apply(lambda t: torch.empty_like(t, device=device))  # 返回的为空的tensor而抛弃数值"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
