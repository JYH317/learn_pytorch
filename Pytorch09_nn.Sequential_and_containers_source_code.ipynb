{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb5c2d83",
   "metadata": {},
   "source": [
    "## 1.Introduce nn.Module source code(Part III)\n",
    "### Part of the class [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) source code is cited below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecb5e30",
   "metadata": {},
   "source": [
    "#### train()函数 和 eval()函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1cdea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self: T, mode: bool = True) -> T: # 定义模型所用到的很多子module（尤其dropout和BN）都是继承自nn.Module，\n",
    "                                                # 因此都有training这个成员变量，在对model调用train()时也会被改变\n",
    "        r\"\"\"Sets the module in training mode.   \n",
    "\n",
    "        This has any effect only on certain modules. See documentations of\n",
    "        particular modules for details of their behaviors in training/evaluation\n",
    "        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
    "        etc.\n",
    "\n",
    "        Args:\n",
    "            mode (bool): whether to set training mode (``True``) or evaluation\n",
    "                         mode (``False``). Default: ``True``.\n",
    "\n",
    "        Returns:\n",
    "            Module: self\n",
    "        \"\"\"\n",
    "        if not isinstance(mode, bool):\n",
    "            raise ValueError(\"training mode is expected to be boolean\")\n",
    "        self.training = mode  # 对成员变量training赋值mode\n",
    "        for module in self.children(): # 遍历当前模型中所有子模块\n",
    "            module.train(mode) # 将所有子模块赋值相应的mode\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def eval(self: T) -> T:\n",
    "        r\"\"\"Sets the module in evaluation mode.\n",
    "\n",
    "        This has any effect only on certain modules. See documentations of\n",
    "        particular modules for details of their behaviors in training/evaluation\n",
    "        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
    "        etc.\n",
    "\n",
    "        This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
    "\n",
    "        See :ref:`locally-disable-grad-doc` for a comparison between\n",
    "        `.eval()` and several similar mechanisms that may be confused with it.\n",
    "\n",
    "        Returns:\n",
    "            Module: self\n",
    "        \"\"\"\n",
    "        return self.train(False) # train()中的mode设置成False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c8a58",
   "metadata": {},
   "source": [
    "#### requires_grad_()函数 和 zero_grad()函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857fd4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def requires_grad_(self: T, requires_grad: bool = True) -> T:\n",
    "        r\"\"\"Change if autograd should record operations on parameters in this\n",
    "        module.\n",
    "\n",
    "        This method sets the parameters' :attr:`requires_grad` attributes\n",
    "        in-place.\n",
    "\n",
    "        This method is helpful for freezing part of the module for finetuning\n",
    "        or training parts of a model individually (e.g., GAN training).\n",
    "\n",
    "        See :ref:`locally-disable-grad-doc` for a comparison between\n",
    "        `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
    "\n",
    "        Args:\n",
    "            requires_grad (bool): whether autograd should record operations on\n",
    "                                  parameters in this module. Default: ``True``.\n",
    "\n",
    "        Returns:\n",
    "            Module: self\n",
    "        \"\"\"\n",
    "        for p in self.parameters(): # 上节说过parameters()函数与_parameters属性不同，其返回的是所有(包括子module)对参数的迭代器\n",
    "            p.requires_grad_(requires_grad) # 这里的requires_grad_()函数虽然和上面的名字相同，但这里的是tensor的函数，上面的是module的函数，做好区分\n",
    "        return self\n",
    "\n",
    "    def zero_grad(self, set_to_none: bool = False) -> None: # 通常在训练的每一步之前optimizer都要调用一下此函数\n",
    "                                                            # pytorch中梯度不会自动清除而是累加，所以每次更新权重后要手动将梯度清零\n",
    "        r\"\"\"Sets gradients of all model parameters to zero. See similar function\n",
    "        under :class:`torch.optim.Optimizer` for more context.\n",
    "\n",
    "        Args:\n",
    "            set_to_none (bool): instead of setting to zero, set the grads to None.\n",
    "                See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
    "        \"\"\"\n",
    "        if getattr(self, '_is_replica', False):\n",
    "            warnings.warn(\n",
    "                \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \"\n",
    "                \"The parameters are copied (in a differentiable manner) from the original module. \"\n",
    "                \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \"\n",
    "                \"If you need gradients in your forward method, consider using autograd.grad instead.\")\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.grad is not None:\n",
    "                if set_to_none:\n",
    "                    p.grad = None\n",
    "                else:\n",
    "                    if p.grad.grad_fn is not None:\n",
    "                        p.grad.detach_()\n",
    "                    else:\n",
    "                        p.grad.requires_grad_(False)\n",
    "                    p.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2326469f",
   "metadata": {},
   "source": [
    "#### \\_\\_repr__()函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c357951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __repr__(self): # 魔法函数\n",
    "        # We treat the extra repr like the sub-module, one item per line\n",
    "        extra_lines = []\n",
    "        extra_repr = self.extra_repr()\n",
    "        # empty string will be split into list ['']\n",
    "        if extra_repr:\n",
    "            extra_lines = extra_repr.split('\\n')\n",
    "        child_lines = []\n",
    "        for key, module in self._modules.items():\n",
    "            mod_str = repr(module)\n",
    "            mod_str = _addindent(mod_str, 2)\n",
    "            child_lines.append('(' + key + '): ' + mod_str)\n",
    "        lines = extra_lines + child_lines\n",
    "\n",
    "        main_str = self._get_name() + '('  # _get_name()返回的就是当前的类名（下面例子中的test）\n",
    "        if lines:  # lines指的是每个module的名称以及描述\n",
    "            # simple one-liner info, which most builtin Modules will use\n",
    "            if len(extra_lines) == 1 and not child_lines:\n",
    "                main_str += extra_lines[0]\n",
    "            else:\n",
    "                main_str += '\\n  ' + '\\n  '.join(lines) + '\\n'\n",
    "\n",
    "        main_str += ')'\n",
    "        return main_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0ebc62",
   "metadata": {},
   "source": [
    "\\_\\_repr__()函数示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebcddead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2d48da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(torch.nn.Module): # 自定义一个module\n",
    "    def __init__(self):\n",
    "        super(Test, self).__init__() # 在子类的init函数定义时一般要调用父类的init函数\n",
    "        self.linear1 = torch.nn.Linear(2, 3)\n",
    "        self.linear2 = torch.nn.Linear(3, 4)\n",
    "        self.batch_norm = torch.nn.BatchNorm2d(4)\n",
    "\n",
    "test_module = Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64cdbfde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Test(\\n  (linear1): Linear(in_features=2, out_features=3, bias=True)\\n  (linear2): Linear(in_features=3, out_features=4, bias=True)\\n  (batch_norm): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(test_module)  # 调用str会返回如下格式是上面的魔法函数__repr__()定义的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "308a791b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Test(\\n  (linear1): Linear(in_features=2, out_features=3, bias=True)\\n  (linear2): Linear(in_features=3, out_features=4, bias=True)\\n  (batch_norm): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\\n)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(test_module) # repr返回对象的规范字符串表示形式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ce33d2",
   "metadata": {},
   "source": [
    "#### \\_\\_dir__()函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b5ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 注意，通过 dir() 函数，不仅仅输出本类中新添加的属性名和方法，还会输出从父类继承得到的属性名和方法名。\n",
    "    def __dir__(self):\n",
    "        module_attrs = dir(self.__class__) # 当前类的dir\n",
    "        attrs = list(self.__dict__.keys()) # 当前字典的所有键值\n",
    "        parameters = list(self._parameters.keys()) # 当前类（不包括子module）参数的键值\n",
    "        modules = list(self._modules.keys()) # module的键值\n",
    "        buffers = list(self._buffers.keys()) # 当前类（不包括子module）buffers的键值\n",
    "        keys = module_attrs + attrs + parameters + modules + buffers\n",
    "\n",
    "        # Eliminate attrs that are not legal Python variable names\n",
    "        keys = [key for key in keys if not key[0].isdigit()]\n",
    "\n",
    "        return sorted(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f8721",
   "metadata": {},
   "source": [
    "\\_\\_dir__()函数示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12f2e41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_backward_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'batch_norm',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'linear1',\n",
       " 'linear2',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(test_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c0967f",
   "metadata": {},
   "source": [
    "## 2.Introduce the source code of classes in Contain.py\n",
    "### [Sequential](https://pytorch.org/tutorials/beginner/nn_tutorial.html#nn-sequential) class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed45344",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):  # 实例化时传入的module的顺序不是随意的而是要有序的\n",
    "    r\"\"\"A sequential container.\n",
    "    Modules will be added to it in the order they are passed in the\n",
    "    constructor. Alternatively, an ``OrderedDict`` of modules can be\n",
    "    passed in. The ``forward()`` method of ``Sequential`` accepts any\n",
    "    input and forwards it to the first module it contains. It then\n",
    "    \"chains\" outputs to inputs sequentially for each subsequent module,\n",
    "    finally returning the output of the last module.\n",
    "\n",
    "    The value a ``Sequential`` provides over manually calling a sequence\n",
    "    of modules is that it allows treating the whole container as a\n",
    "    single module, such that performing a transformation on the\n",
    "    ``Sequential`` applies to each of the modules it stores (which are\n",
    "    each a registered submodule of the ``Sequential``).\n",
    "\n",
    "    What's the difference between a ``Sequential`` and a\n",
    "    :class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it\n",
    "    sounds like--a list for storing ``Module`` s! On the other hand,\n",
    "    the layers in a ``Sequential`` are connected in a cascading way.\n",
    "\n",
    "    Example::\n",
    "\n",
    "        # Using Sequential to create a small model. When `model` is run,\n",
    "        # input will first be passed to `Conv2d(1,20,5)`. The output of\n",
    "        # `Conv2d(1,20,5)` will be used as the input to the first\n",
    "        # `ReLU`; the output of the first `ReLU` will become the input\n",
    "        # for `Conv2d(20,64,5)`. Finally, the output of\n",
    "        # `Conv2d(20,64,5)` will be used as input to the second `ReLU`\n",
    "        model = nn.Sequential(         # 示例1:直接传入子module的实例\n",
    "                  nn.Conv2d(1,20,5),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(20,64,5),\n",
    "                  nn.ReLU()\n",
    "                )\n",
    "\n",
    "        # Using Sequential with OrderedDict. This is functionally the\n",
    "        # same as the above code\n",
    "        model = nn.Sequential(OrderedDict([  # 示例2:传入有序字典，包含子module的名称和实例\n",
    "                  ('conv1', nn.Conv2d(1,20,5)),\n",
    "                  ('relu1', nn.ReLU()),\n",
    "                  ('conv2', nn.Conv2d(20,64,5)),\n",
    "                  ('relu2', nn.ReLU())\n",
    "                ]))\n",
    "    \"\"\"\n",
    "    _modules: Dict[str, Module]  # type: ignore[assignment]\n",
    "\n",
    "    @overload\n",
    "    def __init__(self, *args: Module) -> None:\n",
    "        ...\n",
    "\n",
    "    @overload\n",
    "    def __init__(self, arg: 'OrderedDict[str, Module]') -> None:\n",
    "        ...\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        super(Sequential, self).__init__()\n",
    "        if len(args) == 1 and isinstance(args[0], OrderedDict): # 如果传入的是orderedDict\n",
    "            for key, module in args[0].items():   # 对有序字典进行遍历\n",
    "                self.add_module(key, module)  # 调用add_module()来向self._modules的字典中添加子module，此时module名称为字典中的键\n",
    "        else:\n",
    "            for idx, module in enumerate(args): # 如果直接传入的是子模块的实例\n",
    "                self.add_module(str(idx), module) # 此时的module的名称为子module添加的顺序序号的字符'0'，'1'...\n",
    "\n",
    "    def _get_item_by_idx(self, iterator, idx) -> T:\n",
    "        \"\"\"Get the idx-th item of the iterator\"\"\"\n",
    "        size = len(self)\n",
    "        idx = operator.index(idx)\n",
    "        if not -size <= idx < size:\n",
    "            raise IndexError('index {} is out of range'.format(idx))\n",
    "        idx %= size\n",
    "        return next(islice(iterator, idx, None))\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    def __getitem__(self, idx) -> Union['Sequential', T]:\n",
    "        if isinstance(idx, slice):\n",
    "            return self.__class__(OrderedDict(list(self._modules.items())[idx]))\n",
    "        else:\n",
    "            return self._get_item_by_idx(self._modules.values(), idx)\n",
    "\n",
    "    def __setitem__(self, idx: int, module: Module) -> None:\n",
    "        key: str = self._get_item_by_idx(self._modules.keys(), idx)\n",
    "        return setattr(self, key, module)\n",
    "\n",
    "    def __delitem__(self, idx: Union[slice, int]) -> None:\n",
    "        if isinstance(idx, slice):\n",
    "            for key in list(self._modules.keys())[idx]:\n",
    "                delattr(self, key)\n",
    "        else:\n",
    "            key = self._get_item_by_idx(self._modules.keys(), idx)\n",
    "            delattr(self, key)\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._modules)\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    def __dir__(self):\n",
    "        keys = super(Sequential, self).__dir__()\n",
    "        keys = [key for key in keys if not key.isdigit()]\n",
    "        return keys\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    def __iter__(self) -> Iterator[Module]:\n",
    "        return iter(self._modules.values())\n",
    "\n",
    "    # NB: We can't really type check this function as the type of input\n",
    "    # may change dynamically (as is tested in\n",
    "    # TestScript.test_sequential_intermediary_types).  Cannot annotate\n",
    "    # with Any as TorchScript expects a more precise type\n",
    "    def forward(self, input): \n",
    "        for module in self:\n",
    "            input = module(input) # 当input传入实例化后的model后，input依次地被sequential中的各个子module作用\n",
    "        return input\n",
    "\n",
    "    def append(self, module: Module) -> 'Sequential':\n",
    "        r\"\"\"Appends a given module to the end.\n",
    "\n",
    "        Args:\n",
    "            module (nn.Module): module to append\n",
    "        \"\"\"\n",
    "        self.add_module(str(len(self)), module)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c874aa",
   "metadata": {},
   "source": [
    "nn.Sequential类的示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "613aee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.nn.Sequential(torch.nn.Linear(2,3), torch.nn.Linear(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75c4118e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=3, bias=True)\n",
       "  (1): Linear(in_features=3, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "421c243b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0', Linear(in_features=2, out_features=3, bias=True)),\n",
       "             ('1', Linear(in_features=3, out_features=4, bias=True))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s._modules # _modules返回的子module字典中的键分别为‘0’，‘1’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98bcfcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "s = torch.nn.Sequential(OrderedDict([  \n",
    "                  ('Linear0', torch.nn.Linear(2,3)),\n",
    "                  ('Linear1', torch.nn.Linear(3,4))\n",
    "                ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "121a6628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (Linear0): Linear(in_features=2, out_features=3, bias=True)\n",
       "  (Linear1): Linear(in_features=3, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s  # module的键值为自定义的了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d226dd80",
   "metadata": {},
   "source": [
    "### [ModuleList](https://pytorch.org/docs/master/generated/torch.nn.ModuleList.html#modulelist) class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a1cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelList虽然称作modulelist，但是仍是继承自Module的一个类，继承了很多Module的方法，Module的一些属性和函数其也可调用；与直接将一些子module放进python的列表中差距甚大，注意辨别\n",
    "class ModuleList(Module):     # 将很多module放到一个列表当中\n",
    "    r\"\"\"Holds submodules in a list.\n",
    "\n",
    "    :class:`~torch.nn.ModuleList` can be indexed like a regular Python list, but\n",
    "    modules it contains are properly registered, and will be visible by all\n",
    "    :class:`~torch.nn.Module` methods.\n",
    "\n",
    "    Args:\n",
    "        modules (iterable, optional): an iterable of modules to add\n",
    "\n",
    "    Example::    # 示例\n",
    "\n",
    "        class MyModule(nn.Module):   # 自定义module \n",
    "            def __init__(self):\n",
    "                super(MyModule, self).__init__()\n",
    "                self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)]) # self.Linears是对ModuleList的示例化\n",
    "                                                                                     # ModuleList传入的是由多个子module构成的列表\n",
    "\n",
    "            def forward(self, x):\n",
    "                # ModuleList can act as an iterable, or be indexed using ints\n",
    "                for i, l in enumerate(self.linears):  # 对ModuleList类的实例self.Linears进行遍历\n",
    "                    x = self.linears[i // 2](x) + l(x)\n",
    "                return x\n",
    "    \"\"\"\n",
    "\n",
    "    _modules: Dict[str, Module]  # type: ignore[assignment]\n",
    "\n",
    "    def __init__(self, modules: Optional[Iterable[Module]] = None) -> None:    # ‘modules’传入的是一个module构成的列表 \n",
    "        super(ModuleList, self).__init__()\n",
    "        if modules is not None:\n",
    "            self += modules      # 将传入的modules加入自身中 \n",
    "\n",
    "    def _get_abs_string_index(self, idx):\n",
    "        \"\"\"Get the absolute index for the list of modules\"\"\"\n",
    "        idx = operator.index(idx)\n",
    "        if not (-len(self) <= idx < len(self)):\n",
    "            raise IndexError('index {} is out of range'.format(idx))\n",
    "        if idx < 0:\n",
    "            idx += len(self)\n",
    "        return str(idx)\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    def __getitem__(self, idx: int) -> Union[Module, 'ModuleList']:\n",
    "        if isinstance(idx, slice):\n",
    "            return self.__class__(list(self._modules.values())[idx])\n",
    "        else:\n",
    "            return self._modules[self._get_abs_string_index(idx)]\n",
    "\n",
    "    def __setitem__(self, idx: int, module: Module) -> None:\n",
    "        idx = self._get_abs_string_index(idx)\n",
    "        return setattr(self, str(idx), module)\n",
    "\n",
    "    def __delitem__(self, idx: Union[int, slice]) -> None:\n",
    "        if isinstance(idx, slice):\n",
    "            for k in range(len(self._modules))[idx]:\n",
    "                delattr(self, str(k))\n",
    "        else:\n",
    "            delattr(self, self._get_abs_string_index(idx))\n",
    "        # To preserve numbering, self._modules is being reconstructed with modules after deletion\n",
    "        str_indices = [str(i) for i in range(len(self._modules))]\n",
    "        self._modules = OrderedDict(list(zip(str_indices, self._modules.values())))\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._modules)\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    def __iter__(self) -> Iterator[Module]:\n",
    "        return iter(self._modules.values())\n",
    "\n",
    "    def __iadd__(self, modules: Iterable[Module]) -> 'ModuleList':\n",
    "        return self.extend(modules)\n",
    "\n",
    "    def __add__(self, other: Iterable[Module]) -> 'ModuleList':\n",
    "        combined = ModuleList()\n",
    "        for i, module in enumerate(chain(self, other)):\n",
    "            combined.add_module(str(i), module)\n",
    "        return combined\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    def __dir__(self):\n",
    "        keys = super(ModuleList, self).__dir__()\n",
    "        keys = [key for key in keys if not key.isdigit()]\n",
    "        return keys\n",
    "\n",
    "    def insert(self, index: int, module: Module) -> None:  # 插入一个module\n",
    "        r\"\"\"Insert a given module before a given index in the list.\n",
    "\n",
    "        Args:\n",
    "            index (int): index to insert.\n",
    "            module (nn.Module): module to insert\n",
    "        \"\"\"\n",
    "        for i in range(len(self._modules), index, -1):\n",
    "            self._modules[str(i)] = self._modules[str(i - 1)]\n",
    "        self._modules[str(index)] = module\n",
    "\n",
    "    def append(self, module: Module) -> 'ModuleList':\n",
    "        r\"\"\"Appends a given module to the end of the list.\n",
    "\n",
    "        Args:\n",
    "            module (nn.Module): module to append\n",
    "        \"\"\"\n",
    "        self.add_module(str(len(self)), module)\n",
    "        return self\n",
    "\n",
    "    def extend(self, modules: Iterable[Module]) -> 'ModuleList':\n",
    "        r\"\"\"Appends modules from a Python iterable to the end of the list.\n",
    "\n",
    "        Args:\n",
    "            modules (iterable): iterable of modules to append\n",
    "        \"\"\"\n",
    "        if not isinstance(modules, container_abcs.Iterable):\n",
    "            raise TypeError(\"ModuleList.extend should be called with an \"\n",
    "                            \"iterable, but got \" + type(modules).__name__)\n",
    "        offset = len(self)\n",
    "        for i, module in enumerate(modules):\n",
    "            self.add_module(str(offset + i), module)\n",
    "        return self\n",
    "\n",
    "    # remove forward alltogether to fallback on Module's _forward_unimplemented\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa5beff",
   "metadata": {},
   "source": [
    "### [ModuleDict](https://pytorch.org/docs/master/generated/torch.nn.ModuleDict.html#moduledict) class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd2048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleDict(Module):\n",
    "    r\"\"\"Holds submodules in a dictionary.\n",
    "\n",
    "    :class:`~torch.nn.ModuleDict` can be indexed like a regular Python dictionary,\n",
    "    but modules it contains are properly registered, and will be visible by all\n",
    "    :class:`~torch.nn.Module` methods.\n",
    "\n",
    "    :class:`~torch.nn.ModuleDict` is an **ordered** dictionary that respects\n",
    "\n",
    "    * the order of insertion, and\n",
    "\n",
    "    * in :meth:`~torch.nn.ModuleDict.update`, the order of the merged\n",
    "      ``OrderedDict``, ``dict`` (started from Python 3.6) or another\n",
    "      :class:`~torch.nn.ModuleDict` (the argument to\n",
    "      :meth:`~torch.nn.ModuleDict.update`).\n",
    "\n",
    "    Note that :meth:`~torch.nn.ModuleDict.update` with other unordered mapping\n",
    "    types (e.g., Python's plain ``dict`` before Python version 3.6) does not\n",
    "    preserve the order of the merged mapping.\n",
    "\n",
    "    Args:\n",
    "        modules (iterable, optional): a mapping (dictionary) of (string: module)\n",
    "            or an iterable of key-value pairs of type (string, module)\n",
    "\n",
    "    Example::   # 示例\n",
    "\n",
    "        class MyModule(nn.Module):   # 自定义module\n",
    "            def __init__(self):\n",
    "                super(MyModule, self).__init__()\n",
    "                self.choices = nn.ModuleDict({          # self.choice为ModuleDict的实例化对象\n",
    "                        'conv': nn.Conv2d(10, 10, 3),   # ModuleDict的实例化中传入了两个子module实例化的键值对\n",
    "                        'pool': nn.MaxPool2d(3)\n",
    "                })\n",
    "                self.activations = nn.ModuleDict([      # self.activations操作同上\n",
    "                        ['lrelu', nn.LeakyReLU()],\n",
    "                        ['prelu', nn.PReLU()]\n",
    "                ])\n",
    "\n",
    "            def forward(self, x, choice, act):     # forward中要额外的传入两个键choice和act，去从对应的\n",
    "                x = self.choices[choice](x)        # ModuleDict中索引出相应的子module来使用\n",
    "                x = self.activations[act](x)\n",
    "                return x\n",
    "    \"\"\"\n",
    "\n",
    "    _modules: Dict[str, Module]  # type: ignore[assignment]\n",
    "\n",
    "    def __init__(self, modules: Optional[Mapping[str, Module]] = None) -> None:\n",
    "        super(ModuleDict, self).__init__()\n",
    "        if modules is not None:\n",
    "            self.update(modules)\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    def __getitem__(self, key: str) -> Module:\n",
    "        return self._modules[key]\n",
    "\n",
    "    def __setitem__(self, key: str, module: Module) -> None:\n",
    "        self.add_module(key, module)\n",
    "\n",
    "    def __delitem__(self, key: str) -> None:\n",
    "        del self._modules[key]\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._modules)\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    def __iter__(self) -> Iterator[str]:\n",
    "        return iter(self._modules)\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    def __contains__(self, key: str) -> bool:\n",
    "        return key in self._modules\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Remove all items from the ModuleDict.\n",
    "        \"\"\"\n",
    "        self._modules.clear()\n",
    "\n",
    "    def pop(self, key: str) -> Module:\n",
    "        r\"\"\"Remove key from the ModuleDict and return its module.\n",
    "\n",
    "        Args:\n",
    "            key (string): key to pop from the ModuleDict\n",
    "        \"\"\"\n",
    "        v = self[key]\n",
    "        del self[key]\n",
    "        return v\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    def keys(self) -> Iterable[str]:\n",
    "        r\"\"\"Return an iterable of the ModuleDict keys.\n",
    "        \"\"\"\n",
    "        return self._modules.keys()\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    def items(self) -> Iterable[Tuple[str, Module]]:\n",
    "        r\"\"\"Return an iterable of the ModuleDict key/value pairs.\n",
    "        \"\"\"\n",
    "        return self._modules.items()\n",
    "\n",
    "    @_copy_to_script_wrapper\n",
    "    def values(self) -> Iterable[Module]:\n",
    "        r\"\"\"Return an iterable of the ModuleDict values.\n",
    "        \"\"\"\n",
    "        return self._modules.values()\n",
    "\n",
    "    def update(self, modules: Mapping[str, Module]) -> None:\n",
    "        r\"\"\"Update the :class:`~torch.nn.ModuleDict` with the key-value pairs from a\n",
    "        mapping or an iterable, overwriting existing keys.\n",
    "\n",
    "        .. note::\n",
    "            If :attr:`modules` is an ``OrderedDict``, a :class:`~torch.nn.ModuleDict`, or\n",
    "            an iterable of key-value pairs, the order of new elements in it is preserved.\n",
    "\n",
    "        Args:\n",
    "            modules (iterable): a mapping (dictionary) from string to :class:`~torch.nn.Module`,\n",
    "                or an iterable of key-value pairs of type (string, :class:`~torch.nn.Module`)\n",
    "        \"\"\"\n",
    "        if not isinstance(modules, container_abcs.Iterable):\n",
    "            raise TypeError(\"ModuleDict.update should be called with an \"\n",
    "                            \"iterable of key/value pairs, but got \" +\n",
    "                            type(modules).__name__)\n",
    "\n",
    "        if isinstance(modules, (OrderedDict, ModuleDict, container_abcs.Mapping)):\n",
    "            for key, module in modules.items():\n",
    "                self[key] = module\n",
    "        else:\n",
    "            # modules here can be a list with two items\n",
    "            for j, m in enumerate(modules):\n",
    "                if not isinstance(m, container_abcs.Iterable):\n",
    "                    raise TypeError(\"ModuleDict update sequence element \"\n",
    "                                    \"#\" + str(j) + \" should be Iterable; is\" +\n",
    "                                    type(m).__name__)\n",
    "                if not len(m) == 2:\n",
    "                    raise ValueError(\"ModuleDict update sequence element \"\n",
    "                                     \"#\" + str(j) + \" has length \" + str(len(m)) +\n",
    "                                     \"; 2 is required\")\n",
    "                # modules can be Mapping (what it's typed at), or a list: [(name1, module1), (name2, module2)]\n",
    "                # that's too cumbersome to type correctly with overloads, so we add an ignore here\n",
    "                self[m[0]] = m[1]  # type: ignore[assignment]\n",
    "\n",
    "    # remove forward alltogether to fallback on Module's _forward_unimplemented\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e488bea9",
   "metadata": {},
   "source": [
    "### [ParameterList](https://pytorch.org/docs/master/generated/torch.nn.ParameterList.html#parameterlist) class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25846609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ParameterList依然是继承自nn.Module的类，其实例化的对象依然可以调用nn.Module类的方法，与python list有本质上的不同\n",
    "class ParameterList(Module):\n",
    "    r\"\"\"Holds parameters in a list.\n",
    "\n",
    "    :class:`~torch.nn.ParameterList` can be indexed like a regular Python\n",
    "    list, but parameters it contains are properly registered, and will be\n",
    "    visible by all :class:`~torch.nn.Module` methods.\n",
    "\n",
    "    Args:\n",
    "        parameters (iterable, optional): an iterable of :class:`~torch.nn.Parameter` to add\n",
    "\n",
    "    Example::     # 示例\n",
    "\n",
    "        class MyModule(nn.Module):  # 自定义module\n",
    "            def __init__(self):\n",
    "                super(MyModule, self).__init__()\n",
    "                self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])\n",
    "                                # self.params是对ParameterList的示例化\n",
    "                                # ParameterList传入的是由10个实例化的(10*10的)Parameter类型的对象(非tensor类型)构成的列表\n",
    "\n",
    "            def forward(self, x):\n",
    "                # ParameterList can act as an iterable, or be indexed using ints\n",
    "                for i, p in enumerate(self.params):  # 遍历\n",
    "                    x = self.params[i // 2].mm(x) + p.mm(x) 使用\n",
    "                return x\n",
    "    \"\"\"\n",
    "\n",
    "    _parameters: Dict[str, 'Parameter']  # type: ignore[assignment]\n",
    "\n",
    "    def __init__(self, parameters: Optional[Iterable['Parameter']] = None) -> None:\n",
    "        super(ParameterList, self).__init__()\n",
    "        self._initialized = True\n",
    "        if parameters is not None:\n",
    "            self += parameters\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        state['_initialized'] = False\n",
    "        super(ParameterList, self).__setstate__(state)\n",
    "        self._initialized = True\n",
    "\n",
    "    def _get_abs_string_index(self, idx):\n",
    "        \"\"\"Get the absolute index for the list of modules\"\"\"\n",
    "        idx = operator.index(idx)\n",
    "        if not (-len(self) <= idx < len(self)):\n",
    "            raise IndexError('index {} is out of range'.format(idx))\n",
    "        if idx < 0:\n",
    "            idx += len(self)\n",
    "        return str(idx)\n",
    "\n",
    "    @overload\n",
    "    def __getitem__(self, idx: int) -> 'Parameter':\n",
    "        ...\n",
    "\n",
    "    @overload\n",
    "    def __getitem__(self: T, idx: slice) -> T:\n",
    "        ...\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            return self.__class__(list(self._parameters.values())[idx])\n",
    "        else:\n",
    "            idx = self._get_abs_string_index(idx)\n",
    "            return self._parameters[str(idx)]\n",
    "\n",
    "    def __setitem__(self, idx: int, param: 'Parameter') -> None:\n",
    "        idx = self._get_abs_string_index(idx)\n",
    "        return self.register_parameter(str(idx), param)\n",
    "\n",
    "    def __setattr__(self, key: Any, value: Any) -> None:\n",
    "        if getattr(self, \"_initialized\", False):\n",
    "            if not hasattr(self, key) and not isinstance(value, torch.nn.Parameter):\n",
    "                warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n",
    "        super(ParameterList, self).__setattr__(key, value)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._parameters)\n",
    "\n",
    "    def __iter__(self) -> Iterator['Parameter']:\n",
    "        return iter(self._parameters.values())\n",
    "\n",
    "    def __iadd__(self, parameters: Iterable['Parameter']) -> 'ParameterList':\n",
    "        return self.extend(parameters)\n",
    "\n",
    "    def __dir__(self):\n",
    "        keys = super(ParameterList, self).__dir__()\n",
    "        keys = [key for key in keys if not key.isdigit()]\n",
    "        return keys\n",
    "\n",
    "    def append(self, parameter: 'Parameter') -> 'ParameterList':\n",
    "        \"\"\"Appends a given parameter at the end of the list.\n",
    "\n",
    "        Args:\n",
    "            parameter (nn.Parameter): parameter to append\n",
    "        \"\"\"\n",
    "        self.register_parameter(str(len(self)), parameter)\n",
    "        return self\n",
    "\n",
    "    def extend(self, parameters: Iterable['Parameter']) -> 'ParameterList':\n",
    "        \"\"\"Appends parameters from a Python iterable to the end of the list.\n",
    "\n",
    "        Args:\n",
    "            parameters (iterable): iterable of parameters to append\n",
    "        \"\"\"\n",
    "        if not isinstance(parameters, container_abcs.Iterable):\n",
    "            raise TypeError(\"ParameterList.extend should be called with an \"\n",
    "                            \"iterable, but got \" + type(parameters).__name__)\n",
    "        offset = len(self)\n",
    "        for i, param in enumerate(parameters):\n",
    "            self.register_parameter(str(offset + i), param)\n",
    "        return self\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        child_lines = []\n",
    "        for k, p in self._parameters.items():\n",
    "            size_str = 'x'.join(str(size) for size in p.size())\n",
    "            device_str = '' if not p.is_cuda else ' (GPU {})'.format(p.get_device())\n",
    "            parastr = 'Parameter containing: [{} of size {}{}]'.format(\n",
    "                torch.typename(p), size_str, device_str)\n",
    "            child_lines.append('  (' + str(k) + '): ' + parastr)\n",
    "        tmpstr = '\\n'.join(child_lines)\n",
    "        return tmpstr\n",
    "\n",
    "    def __call__(self, input):\n",
    "        raise RuntimeError('ParameterList should not be called.')\n",
    "\n",
    "    def _replicate_for_data_parallel(self):\n",
    "        warnings.warn(\"nn.ParameterList is being used with DataParallel but this is not \"\n",
    "                      \"supported. This list will appear empty for the models replicated \"\n",
    "                      \"on each GPU except the original one.\")\n",
    "\n",
    "        return super(ParameterList, self)._replicate_for_data_parallel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2755a6a",
   "metadata": {},
   "source": [
    "### [ParameterDict](https://pytorch.org/docs/master/generated/torch.nn.ParameterDict.html#parameterdict) class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8649214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 与ModuleDict的套路类似，这里不再赘述了\n",
    "class ParameterDict(Module):\n",
    "    r\"\"\"Holds parameters in a dictionary.\n",
    "\n",
    "    ParameterDict can be indexed like a regular Python dictionary, but parameters it\n",
    "    contains are properly registered, and will be visible by all Module methods.\n",
    "\n",
    "    :class:`~torch.nn.ParameterDict` is an **ordered** dictionary that respects\n",
    "\n",
    "    * the order of insertion, and\n",
    "\n",
    "    * in :meth:`~torch.nn.ParameterDict.update`, the order of the merged ``OrderedDict``\n",
    "      or another :class:`~torch.nn.ParameterDict` (the argument to\n",
    "      :meth:`~torch.nn.ParameterDict.update`).\n",
    "\n",
    "    Note that :meth:`~torch.nn.ParameterDict.update` with other unordered mapping\n",
    "    types (e.g., Python's plain ``dict``) does not preserve the order of the\n",
    "    merged mapping.\n",
    "\n",
    "    Args:\n",
    "        parameters (iterable, optional): a mapping (dictionary) of\n",
    "            (string : :class:`~torch.nn.Parameter`) or an iterable of key-value pairs\n",
    "            of type (string, :class:`~torch.nn.Parameter`)\n",
    "\n",
    "    Example::\n",
    "\n",
    "        class MyModule(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(MyModule, self).__init__()\n",
    "                self.params = nn.ParameterDict({\n",
    "                        'left': nn.Parameter(torch.randn(5, 10)),\n",
    "                        'right': nn.Parameter(torch.randn(5, 10))\n",
    "                })\n",
    "\n",
    "            def forward(self, x, choice):\n",
    "                x = self.params[choice].mm(x)\n",
    "                return x\n",
    "    \"\"\"\n",
    "\n",
    "    _parameters: Dict[str, 'Parameter']  # type: ignore[assignment]\n",
    "\n",
    "    def __init__(self, parameters: Optional[Mapping[str, 'Parameter']] = None) -> None:\n",
    "        super(ParameterDict, self).__init__()\n",
    "        self._initialized = True\n",
    "        if parameters is not None:\n",
    "            self.update(parameters)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        state['_initialized'] = False\n",
    "        super(ParameterDict, self).__setstate__(state)\n",
    "        self._initialized = True\n",
    "\n",
    "    def __getitem__(self, key: str) -> 'Parameter':\n",
    "        return self._parameters[key]\n",
    "\n",
    "    def __setitem__(self, key: str, parameter: 'Parameter') -> None:\n",
    "        self.register_parameter(key, parameter)\n",
    "\n",
    "    def __delitem__(self, key: str) -> None:\n",
    "        del self._parameters[key]\n",
    "\n",
    "    def __setattr__(self, key: Any, value: Any) -> None:\n",
    "        if getattr(self, \"_initialized\", False):\n",
    "            if not hasattr(self, key) and not isinstance(value, torch.nn.Parameter):\n",
    "                warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n",
    "        super(ParameterDict, self).__setattr__(key, value)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._parameters)\n",
    "\n",
    "    def __iter__(self) -> Iterator[str]:\n",
    "        return iter(self._parameters.keys())\n",
    "\n",
    "    def __reversed__(self) -> Iterator[str]:\n",
    "        return reversed(list(self._parameters.keys()))\n",
    "\n",
    "    def copy(self) -> 'ParameterDict':\n",
    "        \"\"\"Returns a copy of this :class:`~torch.nn.ParameterDict` instance.\n",
    "        \"\"\"\n",
    "        return ParameterDict(self._parameters.copy())\n",
    "\n",
    "    def __contains__(self, key: str) -> bool:\n",
    "        return key in self._parameters\n",
    "\n",
    "    def setdefault(self, key: str, default: Optional['Parameter'] = None) -> 'Parameter':\n",
    "        \"\"\"If key is in the ParameterDict, return its parameter.\n",
    "        If not, insert `key` with a parameter `default` and return `default`.\n",
    "        `default` defaults to `None`.\n",
    "\n",
    "        Args:\n",
    "            key (string): key to set default for\n",
    "            default (:class:`~torch.nn.Parameter`): the parameter set to the key\n",
    "        \"\"\"\n",
    "        if key in self._parameters:\n",
    "            return self._parameters[key]\n",
    "        self[key] = default  # type: ignore[assignment]\n",
    "        return self._parameters[key]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Remove all items from the ParameterDict.\n",
    "        \"\"\"\n",
    "        self._parameters.clear()\n",
    "\n",
    "    def pop(self, key: str) -> 'Parameter':\n",
    "        r\"\"\"Remove key from the ParameterDict and return its parameter.\n",
    "\n",
    "        Args:\n",
    "            key (string): key to pop from the ParameterDict\n",
    "        \"\"\"\n",
    "        v = self[key]\n",
    "        del self[key]\n",
    "        return v\n",
    "\n",
    "    def popitem(self) -> Tuple[str, 'Parameter']:\n",
    "        \"\"\"Remove and return the last inserted `(key, parameter)` pair\n",
    "        from the ParameterDict\n",
    "        \"\"\"\n",
    "        return self._parameters.popitem()\n",
    "\n",
    "    def get(self, key: str, default: Optional['Parameter'] = None) -> 'Parameter | None':\n",
    "        r\"\"\"Return the parameter associated with key if present.\n",
    "        Otherwise return default if provided, None if not.\n",
    "\n",
    "        Args:\n",
    "            key (string): key to get from the ParameterDict\n",
    "            default (Parameter, optional): value to return if key not present\n",
    "        \"\"\"\n",
    "        return self._parameters.get(key, default)\n",
    "\n",
    "    def fromkeys(self, keys: Iterable['str'], default: Optional['Parameter'] = None) -> 'ParameterDict':\n",
    "        r\"\"\"Return a new ParameterDict with the keys provided\n",
    "\n",
    "        Args:\n",
    "            keys (iterable, string): keys to make the new ParameterDict from\n",
    "            default (Parameter, optional): value to set for all keys\n",
    "        \"\"\"\n",
    "        return ParameterDict(self._parameters.fromkeys(keys, default))  # type: ignore[arg-type]\n",
    "\n",
    "    def keys(self) -> Iterable[str]:\n",
    "        r\"\"\"Return an iterable of the ParameterDict keys.\n",
    "        \"\"\"\n",
    "        return self._parameters.keys()\n",
    "\n",
    "    def items(self) -> Iterable[Tuple[str, 'Parameter']]:\n",
    "        r\"\"\"Return an iterable of the ParameterDict key/value pairs.\n",
    "        \"\"\"\n",
    "        return self._parameters.items()\n",
    "\n",
    "    def values(self) -> Iterable['Parameter']:\n",
    "        r\"\"\"Return an iterable of the ParameterDict values.\n",
    "        \"\"\"\n",
    "        return self._parameters.values()\n",
    "\n",
    "    def update(self, parameters: Mapping[str, 'Parameter']) -> None:\n",
    "        r\"\"\"Update the :class:`~torch.nn.ParameterDict` with the key-value pairs from a\n",
    "        mapping or an iterable, overwriting existing keys.\n",
    "\n",
    "        .. note::\n",
    "            If :attr:`parameters` is an ``OrderedDict``, a :class:`~torch.nn.ParameterDict`, or\n",
    "            an iterable of key-value pairs, the order of new elements in it is preserved.\n",
    "\n",
    "        Args:\n",
    "            parameters (iterable): a mapping (dictionary) from string to\n",
    "                :class:`~torch.nn.Parameter`, or an iterable of\n",
    "                key-value pairs of type (string, :class:`~torch.nn.Parameter`)\n",
    "        \"\"\"\n",
    "        if not isinstance(parameters, container_abcs.Iterable):\n",
    "            raise TypeError(\"ParametersDict.update should be called with an \"\n",
    "                            \"iterable of key/value pairs, but got \" +\n",
    "                            type(parameters).__name__)\n",
    "\n",
    "        if isinstance(parameters, (OrderedDict, ParameterDict)):\n",
    "            for key, parameter in parameters.items():\n",
    "                self[key] = parameter\n",
    "        elif isinstance(parameters, container_abcs.Mapping):\n",
    "            for key, parameter in sorted(parameters.items()):\n",
    "                self[key] = parameter\n",
    "        else:\n",
    "            for j, p in enumerate(parameters):\n",
    "                if not isinstance(p, container_abcs.Iterable):\n",
    "                    raise TypeError(\"ParameterDict update sequence element \"\n",
    "                                    \"#\" + str(j) + \" should be Iterable; is\" +\n",
    "                                    type(p).__name__)\n",
    "                if not len(p) == 2:\n",
    "                    raise ValueError(\"ParameterDict update sequence element \"\n",
    "                                     \"#\" + str(j) + \" has length \" + str(len(p)) +\n",
    "                                     \"; 2 is required\")\n",
    "                # parameters as length-2 list too cumbersome to type, see ModuleDict.update comment\n",
    "                self[p[0]] = p[1]  # type: ignore[assignment]\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        child_lines = []\n",
    "        for k, p in self._parameters.items():\n",
    "            size_str = 'x'.join(str(size) for size in p.size())\n",
    "            device_str = '' if not p.is_cuda else ' (GPU {})'.format(p.get_device())\n",
    "            parastr = 'Parameter containing: [{} of size {}{}]'.format(\n",
    "                torch.typename(p), size_str, device_str)\n",
    "            child_lines.append('  (' + k + '): ' + parastr)\n",
    "        tmpstr = '\\n'.join(child_lines)\n",
    "        return tmpstr\n",
    "\n",
    "    def __call__(self, input):\n",
    "        raise RuntimeError('ParameterDict should not be called.')\n",
    "\n",
    "    def _replicate_for_data_parallel(self):\n",
    "        warnings.warn(\"nn.ParameterDict is being used with DataParallel but this is not \"\n",
    "                      \"supported. This dict will appear empty for the models replicated \"\n",
    "                      \"on each GPU except the original one.\")\n",
    "\n",
    "        return super(ParameterDict, self)._replicate_for_data_parallel()\n",
    "\n",
    "    def __or__(self, other: 'ParameterDict') -> 'ParameterDict':\n",
    "        copy = self.copy()\n",
    "        copy.update(other._parameters)\n",
    "        return copy\n",
    "\n",
    "    def __ror__(self, other: 'ParameterDict') -> 'ParameterDict':\n",
    "        copy = other.copy()\n",
    "        copy.update(self._parameters)\n",
    "        return copy\n",
    "\n",
    "    def __ior__(self, other : 'ParameterDict') -> 'ParameterDict':\n",
    "        self.update(other._parameters)\n",
    "        return self"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
