{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28472fe7",
   "metadata": {},
   "source": [
    "## [TORCH.AUTOGRAD.FUNCTIONAL.JACOBIAN](https://pytorch.org/docs/master/generated/torch.autograd.functional.jacobian.html#torch-autograd-functional-jacobian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2ac8bb",
   "metadata": {},
   "source": [
    ">### torch.autograd.functional.jacobian** ( ***func, inputs, create_graph=False, strict=False,vectorize=False, strategy='reverse-mode'*** ) [SOURCE](https://pytorch.org/docs/master/_modules/torch/autograd/functional.html#jacobian)  \n",
    "\n",
    "&emsp;&emsp;Function that computes the Jacobian of a given function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cf2c95",
   "metadata": {},
   "source": [
    "#### **Parameters**\n",
    " - **func** (function) – a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.\n",
    " \n",
    " - **inputs** (tuple of Tensors or [<font color=\"red\">Tensor</font>](https://pytorch.org/docs/master/tensors.html#torch.Tensor)) – inputs to the function <font color=\"DarkBlue\">func</font>.\n",
    " \n",
    " - **create_graph** ([<font color=\"red\">bool</font>](https://docs.python.org/3/library/functions.html#bool), optional) – If <font color=\"DarkBlue\">True</font>, the Jacobian will be computed in a differentiable manner. Note that when <font color=\"DarkBlue\">strict</font> is <font color=\"DarkBlue\">False</font>, the result can not require gradients or be disconnected from the inputs. Defaults to <font color=\"DarkBlue\">False</font>.  \n",
    " \n",
    " - **strict** ([<font color=\"red\">bool</font>](https://docs.python.org/3/library/functions.html#bool), optional) – If <font color=\"DarkBlue\">True</font>, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If <font color=\"DarkBlue\">False</font>, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value. Defaults to <font color=\"DarkBlue\">False</font>.\n",
    " \n",
    " - **vectorize** ([<font color=\"red\">bool</font>](https://docs.python.org/3/library/functions.html#bool), optional) – This feature is experimental. Please consider using [<font color=\"red\">functorch’s jacrev or jacfwd</font>](https://github.com/pytorch/functorch#what-are-the-transforms) instead if you are looking for something less experimental and more performant. When computing the jacobian, usually we invoke <font color=\"DarkBlue\">autograd.grad</font> once per row of the jacobian. If this flag is <font color=\"DarkBlue\">True</font>, we perform only a single <font color=\"DarkBlue\">autograd.grad</font> call with <font color=\"DarkBlue\">batched_grad=True</font> which uses the vmap prototype feature. Though this should lead to performance improvements in many cases, because this feature is still experimental, there may be performance cliffs. See [torch.autograd.grad()](https://pytorch.org/docs/master/generated/torch.autograd.grad.html#torch.autograd.grad)’s <font color=\"DarkBlue\">batched_grad</font> parameter for more information.\n",
    " \n",
    " - **strategy** ([<font color=\"red\">str</font>](https://docs.python.org/3/library/stdtypes.html#str), optional) – Set to <font color=\"DarkBlue\">\"forward-mode\"</font> or <font color=\"DarkBlue\">\"reverse-mode\"</font> to determine whether the Jacobian will be computed with forward or reverse mode AD. Currently, <font color=\"DarkBlue\">\"forward-mode\"</font> requires <font color=\"DarkBlue\">vectorized=True</font>. Defaults to <font color=\"DarkBlue\">\"reverse-mode\"</font>. If func has more outputs than inputs, <font color=\"DarkBlue\">\"forward-mode\"</font> tends to be more performant. Otherwise, prefer to use <font color=\"DarkBlue\">\"reverse-mode\"</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f77a30",
   "metadata": {},
   "source": [
    "#### **Returns**  \n",
    "- if there is a single input and output, this will be a single Tensor containing the Jacobian for the linearized inputs and output. If one of the two is a tuple, then the Jacobian will be a tuple of Tensors. If both of them are tuples, then the Jacobian will be a tuple of tuple of Tensors where <font color=\"DarkBlue\">Jacobian[i][j]</font> will contain the Jacobian of the **<font color=\"DarkBlue\">ith</font>** output and **<font color=\"DarkBlue\">jth</font>** input and will have as size the concatenation of the sizes of the corresponding output and the corresponding input and will have same dtype and device as the corresponding input. If strategy is <font color=\"DarkBlue\">forward-mode</font>, the dtype will be that of the output; otherwise, the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d7087",
   "metadata": {},
   "source": [
    "#### Return type\n",
    "- Jacobian ([<font color=\"red\">Tensor</font>](https://pytorch.org/docs/master/tensors.html#torch.Tensor) or nested tuple of Tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34718e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.7033, 2.0317],\n",
       "         [0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000],\n",
       "         [1.1729, 1.0367]]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "import torch\n",
    "from torch.autograd.functional import jacobian\n",
    "\n",
    "def exp_reducer(x): # 定义函数，输入x张量，求e^x并对第一纬求和\n",
    "  return x.exp().sum(dim=1)\n",
    "inputs = torch.rand(2, 2) # rand为0-1均匀分布\n",
    "jacobian(exp_reducer, inputs) # inputs为传入exp_reducer函数的输入, 输出的是exp_reduce(input)对input的求导结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3f01b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.7033, 2.0317],\n",
       "         [0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000],\n",
       "         [1.1729, 1.0367]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian(exp_reducer, inputs, create_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2b3a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4.2750, 0.0000],\n",
       "         [0.0000, 2.2751]]),\n",
       " tensor([[3., 0.],\n",
       "         [0., 3.]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exp_adder(x, y):\n",
    "  return 2 * x.exp() + 3 * y\n",
    "inputs = (torch.rand(2), torch.rand(2))\n",
    "jacobian(exp_adder, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33a2b7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4765, -0.7783, -0.3851],\n",
       "        [-2.1350, -0.5474, -1.8763]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,3)\n",
    "x   # 高斯分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ab0626e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7605, 0.8499])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = exp_reducer(x) # 求e指数并对dim1求和\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e52f86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6210, 0.4592, 0.6804],\n",
       "         [0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000],\n",
       "         [0.1182, 0.5785, 0.1531]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian(exp_reducer, x) # 上面的2*3张量为y1对x的各个分量分别求偏导的结果\n",
    "                         # 下面的2*3张量为y2对x各个分量分别求偏导的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "500b1d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = tensor([-1.3400,  0.3056,  1.1258])\n",
      "x = tensor([-0.6280,  0.0718,  0.2278])\n",
      "a+x = tensor([-1.9680,  0.3773,  1.3535])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]]),\n",
       " torch.Size([3, 3]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 向量对向量求导\n",
    "a = torch.randn(3)\n",
    "print(\"a =\", a)\n",
    "def func(x):\n",
    "    return a+x\n",
    "x = torch.randn(3)\n",
    "print(\"x =\", x)\n",
    "print(\"a+x =\",func(x))\n",
    "jacobian(func, x), jacobian(func, x).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c65a5415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = tensor([[ 0.2130,  0.9812, -1.7295],\n",
      "        [-0.7298, -0.4272, -0.5411],\n",
      "        [ 0.0050,  1.1093,  0.1938]])\n",
      "x = tensor([[-1.9332, -1.1321,  0.5352],\n",
      "        [ 0.0445, -0.2164,  2.1953],\n",
      "        [ 0.0195, -0.6834,  1.1046]])\n",
      "a+x = tensor([[-1.7202, -0.1509, -1.1943],\n",
      "        [-0.6853, -0.6436,  1.6543],\n",
      "        [ 0.0245,  0.4258,  1.2983]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[[1., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 1., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 1.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 1., 0.],\n",
       "           [0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 1.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [1., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 1., 0.]],\n",
       " \n",
       "          [[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 0., 1.]]]]),\n",
       " torch.Size([3, 3, 3, 3]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵对矩阵求导\n",
    "a = torch.randn(3,3)\n",
    "print(\"a =\", a)\n",
    "def func(x):\n",
    "    return a+x\n",
    "x = torch.randn(3,3)\n",
    "print(\"x =\", x)\n",
    "print(\"a+x =\",func(x))\n",
    "jacobian(func, x), jacobian(func, x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f58b69ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = tensor([ 0.1043, -0.0819, -0.9433])\n",
      "x = tensor([-0.7089, -0.8569,  0.6958], requires_grad=True)\n",
      "y = tensor([-0.6047, -0.9388, -0.2475], grad_fn=<AddBackward0>)\n",
      "x.grad = tensor([1., 1., 1.])\n",
      "jacobian: tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用.backward()函数计算\n",
    "a = torch.randn(3)\n",
    "print(\"a =\", a)\n",
    "def func(x):\n",
    "    return a+x\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(\"x =\", x)\n",
    "y = func(x)\n",
    "print(\"y =\", y)\n",
    "y.backward(torch.ones_like(y))# y不是标量，如果不传入和y一样size的tensor会报错，传入全1的tensor相当于求y.sun()对x的导数\n",
    "print(\"x.grad =\",x.grad) \n",
    "print(\"jacobian:\", jacobian(func, x))\n",
    "torch.ones_like(y) @ jacobian(func, x) # 矩阵相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a931208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = tensor([[-0.5171, -0.9421,  0.8334],\n",
      "        [-0.0342, -0.8039, -0.1967]], requires_grad=True)\n",
      "b = tensor([[ 0.7732, -0.9926],\n",
      "        [ 0.3547, -0.4250],\n",
      "        [-0.4115,  0.5164]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0770,  1.3441],\n",
       "        [-0.2306,  0.2740]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2, 3, requires_grad=True)\n",
    "print(\"a =\", a)\n",
    "b = torch.randn(3, 2, requires_grad=True)\n",
    "print(\"b =\", b)\n",
    "y = a @ b\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f69215d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py:166\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    162\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \\\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mtuple\u001b[39m(inputs) \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m    165\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 166\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py:67\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 67\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mones_like(out, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format))\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "y.backward()    # y不是标量会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abb47f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad: tensor([[-0.2194, -0.0703,  0.1049],\n",
      "        [-0.2194, -0.0703,  0.1049]])\n",
      "b.grad: tensor([[-0.5513, -0.5513],\n",
      "        [-1.7460, -1.7460],\n",
      "        [ 0.6367,  0.6367]])\n"
     ]
    }
   ],
   "source": [
    "y.backward(torch.ones_like(y))\n",
    "print(\"a.grad:\", a.grad)\n",
    "print(\"b.grad:\", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e71885b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0770,  1.3441],\n",
       "        [-0.2306,  0.2740]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用jacobian函数验证一下\n",
    "def func(a):\n",
    "    return a @ b\n",
    "func(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5ad8e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0770,  1.3441], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func(a[0]) # 验证第一行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e426f40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2194, -0.0703,  0.1049])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(func(a[0])) @ jacobian(func, a[0])   # 参考上一节中a.grad的计算方法：(v.T)·J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f9a90b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2194, -0.0703,  0.1049])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
